#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–µ—Ç—Ä–∏–∫ —Å–∞–π—Ç–∞
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏, SEO –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —Ä–∞–∑–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü –∏ —Ç.–¥.
"""

import argparse
import json
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup


# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
class Colors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    RESET = '\033[0m'
    BOLD = '\033[1m'

class PerformanceChecker:
    def __init__(self, base_url: str, timeout: int = 30):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.results = {}

    def check_url(self, url: str, measure_ttfb: bool = False) -> Tuple[bool, Optional[requests.Response], Optional[float], Optional[float]]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL"""
        try:
            start_time = time.time()
            response = self.session.get(url, timeout=self.timeout, allow_redirects=True, stream=True)
            
            # –ò–∑–º–µ—Ä—è–µ–º TTFB (Time To First Byte)
            ttfb = None
            if measure_ttfb:
                ttfb = time.time() - start_time
                # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
                response.content
            
            load_time = time.time() - start_time
            return True, response, load_time, ttfb
        except requests.exceptions.RequestException as e:
            return False, None, None, None

    def check_page_performance(self, url: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        print(f"\n{Colors.CYAN}üîç –ü—Ä–æ–≤–µ—Ä–∫–∞: {url}{Colors.RESET}")

        success, response, load_time, ttfb = self.check_url(url, measure_ttfb=True)

        if not success or response is None:
            return {
                'url': url,
                'status': 'error',
                'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'
            }

        # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø—Ä–æ—á–∏—Ç–∞–Ω
        if not hasattr(response, '_content') or response._content is None:
            response.content

        result = {
            'url': url,
            'status_code': response.status_code,
            'load_time': round(load_time, 3),
            'ttfb': round(ttfb, 3) if ttfb else None,
            'size': len(response.content),
            'size_kb': round(len(response.content) / 1024, 2),
            'headers': dict(response.headers),
            'redirects': len(response.history),
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
        if response.status_code == 200:
            result['status'] = 'ok'
            print(f"{Colors.GREEN}‚úì –°—Ç–∞—Ç—É—Å: {response.status_code}{Colors.RESET}")
        else:
            result['status'] = 'warning'
            print(f"{Colors.YELLOW}‚ö† –°—Ç–∞—Ç—É—Å: {response.status_code}{Colors.RESET}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ TTFB
        if ttfb:
            if ttfb < 0.2:
                print(f"{Colors.GREEN}‚úì TTFB: {ttfb:.3f}s (–æ—Ç–ª–∏—á–Ω–æ){Colors.RESET}")
            elif ttfb < 0.6:
                print(f"{Colors.YELLOW}‚ö† TTFB: {ttfb:.3f}s (—Ö–æ—Ä–æ—à–æ){Colors.RESET}")
            else:
                print(f"{Colors.RED}‚úó TTFB: {ttfb:.3f}s (–º–µ–¥–ª–µ–Ω–Ω–æ - –≤–æ–∑–º–æ–∂–Ω–æ —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç){Colors.RESET}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        if load_time < 1.0:
            print(f"{Colors.GREEN}‚úì –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.3f}s (–æ—Ç–ª–∏—á–Ω–æ){Colors.RESET}")
        elif load_time < 2.0:
            print(f"{Colors.YELLOW}‚ö† –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.3f}s (—Ö–æ—Ä–æ—à–æ){Colors.RESET}")
        elif load_time < 5.0:
            print(f"{Colors.YELLOW}‚ö† –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.3f}s (–ø—Ä–∏–µ–º–ª–µ–º–æ){Colors.RESET}")
        else:
            print(f"{Colors.RED}‚úó –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {load_time:.3f}s (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ!){Colors.RESET}")
            if ttfb and ttfb > 5.0:
                print(f"  {Colors.YELLOW}üí° –í–æ–∑–º–æ–∂–Ω–æ —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç —Å–µ—Ä–≤–µ—Ä–∞ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞{Colors.RESET}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        size_mb = result['size_kb'] / 1024
        if size_mb < 0.5:
            print(f"{Colors.GREEN}‚úì –†–∞–∑–º–µ—Ä: {result['size_kb']} KB (–æ—Ç–ª–∏—á–Ω–æ){Colors.RESET}")
        elif size_mb < 1.0:
            print(f"{Colors.YELLOW}‚ö† –†–∞–∑–º–µ—Ä: {result['size_kb']} KB (—Ö–æ—Ä–æ—à–æ){Colors.RESET}")
        else:
            print(f"{Colors.RED}‚úó –†–∞–∑–º–µ—Ä: {result['size_kb']} KB (–±–æ–ª—å—à–æ–π){Colors.RESET}")

        # –ü–∞—Ä—Å–∏–Ω–≥ HTML –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            result['seo'] = self.check_seo(soup, url)
            result['performance_hints'] = self.check_performance_hints(soup, response)
            result['resources'] = self.check_resources(soup, url)
        except Exception as e:
            result['parse_error'] = str(e)

        return result

    def check_seo(self, soup: BeautifulSoup, url: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç SEO –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        seo = {
            'title': None,
            'description': None,
            'keywords': None,
            'og_title': None,
            'og_description': None,
            'og_image': None,
            'canonical': None,
            'robots': None,
            'issues': []
        }

        # Title
        title_tag = soup.find('title')
        if title_tag:
            seo['title'] = title_tag.get_text().strip()
            if len(seo['title']) < 30:
                seo['issues'].append('Title —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (< 30 —Å–∏–º–≤–æ–ª–æ–≤)')
            elif len(seo['title']) > 60:
                seo['issues'].append('Title —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (> 60 —Å–∏–º–≤–æ–ª–æ–≤)')
        else:
            seo['issues'].append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–≥ <title>')

        # Meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            seo['description'] = meta_desc.get('content', '').strip()
            if len(seo['description']) < 120:
                seo['issues'].append('Description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (< 120 —Å–∏–º–≤–æ–ª–æ–≤)')
            elif len(seo['description']) > 160:
                seo['issues'].append('Description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (> 160 —Å–∏–º–≤–æ–ª–æ–≤)')
        else:
            seo['issues'].append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description')

        # Keywords
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            seo['keywords'] = meta_keywords.get('content', '').strip()

        # Open Graph
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        if og_title:
            seo['og_title'] = og_title.get('content', '').strip()

        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc:
            seo['og_description'] = og_desc.get('content', '').strip()

        og_image = soup.find('meta', attrs={'property': 'og:image'})
        if og_image:
            seo['og_image'] = og_image.get('content', '').strip()

        # Canonical
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if canonical:
            seo['canonical'] = canonical.get('href', '').strip()

        # Robots
        robots = soup.find('meta', attrs={'name': 'robots'})
        if robots:
            seo['robots'] = robots.get('content', '').strip()

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SEO
        print(f"\n{Colors.BLUE}üìä SEO –ø—Ä–æ–≤–µ—Ä–∫–∞:{Colors.RESET}")
        if seo['title']:
            print(f"  Title: {seo['title'][:60]}...")
        if seo['description']:
            print(f"  Description: {seo['description'][:80]}...")
        if seo['issues']:
            for issue in seo['issues']:
                print(f"  {Colors.YELLOW}‚ö† {issue}{Colors.RESET}")
        else:
            print(f"  {Colors.GREEN}‚úì SEO –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Ä—è–¥–∫–µ{Colors.RESET}")

        return seo

    def check_performance_hints(self, soup: BeautifulSoup, response: requests.Response) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        hints = {
            'images_without_alt': [],
            'images_count': 0,
            'scripts_count': 0,
            'stylesheets_count': 0,
            'videos_count': 0,
            'issues': []
        }

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = soup.find_all('img')
        hints['images_count'] = len(images)
        for img in images:
            if not img.get('alt'):
                src = img.get('src', '')
                hints['images_without_alt'].append(src)
            if not img.get('loading'):
                hints['issues'].append('–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ lazy loading')

        # –°–∫—Ä–∏–ø—Ç—ã
        scripts = soup.find_all('script')
        hints['scripts_count'] = len(scripts)

        # –°—Ç–∏–ª–∏
        stylesheets = soup.find_all('link', attrs={'rel': 'stylesheet'})
        hints['stylesheets_count'] = len(stylesheets)

        # –í–∏–¥–µ–æ
        videos = soup.find_all('video')
        hints['videos_count'] = len(videos)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cache_control = response.headers.get('Cache-Control', '')
        if not cache_control:
            hints['issues'].append('–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç Cache-Control –∑–∞–≥–æ–ª–æ–≤–æ–∫')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∂–∞—Ç–∏—è
        content_encoding = response.headers.get('Content-Encoding', '')
        if 'gzip' not in content_encoding and 'br' not in content_encoding:
            hints['issues'].append('–í–æ–∑–º–æ–∂–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–∂–∞—Ç–∏–µ (gzip/brotli)')

        # –í—ã–≤–æ–¥ –ø–æ–¥—Å–∫–∞–∑–æ–∫
        if hints['issues']:
            print(f"\n{Colors.YELLOW}üí° –ü–æ–¥—Å–∫–∞–∑–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:{Colors.RESET}")
            for issue in hints['issues']:
                print(f"  ‚ö† {issue}")

        return hints

    def check_resources(self, soup: BeautifulSoup, base_url: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã (CSS, JS, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)"""
        resources = {
            'css': [],
            'js': [],
            'images': [],
            'fonts': [],
            'issues': []
        }
        
        # CSS —Ñ–∞–π–ª—ã
        css_links = soup.find_all('link', attrs={'rel': 'stylesheet'})
        for link in css_links:
            href = link.get('href', '')
            if href:
                full_url = urljoin(base_url, href)
                resources['css'].append(full_url)
        
        # JS —Ñ–∞–π–ª—ã
        js_scripts = soup.find_all('script', src=True)
        for script in js_scripts:
            src = script.get('src', '')
            if src:
                full_url = urljoin(base_url, src)
                resources['js'].append(full_url)
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = soup.find_all('img', src=True)
        for img in images:
            src = img.get('src', '')
            if src and not src.startswith('data:'):
                full_url = urljoin(base_url, src)
                resources['images'].append(full_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
        if len(resources['css']) > 5:
            resources['issues'].append(f'–ú–Ω–æ–≥–æ CSS —Ñ–∞–π–ª–æ–≤ ({len(resources["css"])}) - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ')
        if len(resources['js']) > 10:
            resources['issues'].append(f'–ú–Ω–æ–≥–æ JS —Ñ–∞–π–ª–æ–≤ ({len(resources["js"])}) - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ code splitting')
        
        return resources

    def check_api_endpoints(self, endpoints: List[str]) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç API endpoints"""
        print(f"\n{Colors.CYAN}üîå –ü—Ä–æ–≤–µ—Ä–∫–∞ API endpoints:{Colors.RESET}")
        api_results = {}

        for endpoint in endpoints:
            url = urljoin(self.base_url, endpoint)
            print(f"\n  –ü—Ä–æ–≤–µ—Ä–∫–∞: {endpoint}")

            success, response, load_time, _ = self.check_url(url)

            if success and response:
                status_emoji = "‚úì" if response.status_code == 200 else "‚ö†"
                status_color = Colors.GREEN if response.status_code == 200 else Colors.YELLOW
                print(f"  {status_color}{status_emoji} {response.status_code} ({load_time:.3f}s){Colors.RESET}")

                api_results[endpoint] = {
                    'status_code': response.status_code,
                    'load_time': round(load_time, 3),
                    'size': len(response.content)
                }
            else:
                print(f"  {Colors.RED}‚úó –û—à–∏–±–∫–∞{Colors.RESET}")
                api_results[endpoint] = {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å'}

        return api_results

    def check_sitemap(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç sitemap.xml"""
        sitemap_url = urljoin(self.base_url, '/sitemap.xml')
        print(f"\n{Colors.CYAN}üó∫ –ü—Ä–æ–≤–µ—Ä–∫–∞ sitemap.xml:{Colors.RESET}")

        success, response, load_time, _ = self.check_url(sitemap_url)

        if success and response:
            if response.status_code == 200:
                print(f"{Colors.GREEN}‚úì Sitemap –¥–æ—Å—Ç—É–ø–µ–Ω ({load_time:.3f}s){Colors.RESET}")
                try:
                    # –ü–∞—Ä—Å–∏–º sitemap
                    soup = BeautifulSoup(response.text, 'xml')
                    urls = soup.find_all('url')
                    print(f"  –ù–∞–π–¥–µ–Ω–æ URL: {len(urls)}")
                    if urls:
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ URL
                        for i, url_tag in enumerate(urls[:3]):
                            loc = url_tag.find('loc')
                            if loc:
                                print(f"    - {loc.get_text()}")
                    return {
                        'status': 'ok',
                        'urls_count': len(urls),
                        'load_time': round(load_time, 3)
                    }
                except Exception as e:
                    print(f"{Colors.YELLOW}‚ö† –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sitemap: {e}{Colors.RESET}")
                    print(f"  –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:200]}...")
                    return {'status': 'parse_error', 'error': str(e)}
            else:
                print(f"{Colors.YELLOW}‚ö† Sitemap –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}{Colors.RESET}")
                return {'status': 'error', 'status_code': response.status_code}
        else:
            print(f"{Colors.RED}‚úó Sitemap –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ sitemap.ts —Å–æ–∑–¥–∞—ë—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç){Colors.RESET}")
            return {'status': 'error'}

    def check_robots(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç robots.txt"""
        robots_url = urljoin(self.base_url, '/robots.txt')
        print(f"\n{Colors.CYAN}ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt:{Colors.RESET}")

        success, response, load_time, _ = self.check_url(robots_url)

        if success and response and response.status_code == 200:
            print(f"{Colors.GREEN}‚úì Robots.txt –¥–æ—Å—Ç—É–ø–µ–Ω ({load_time:.3f}s){Colors.RESET}")
            print(f"  –°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{response.text[:200]}...")
            return {
                'status': 'ok',
                'load_time': round(load_time, 3)
            }
        else:
            print(f"{Colors.YELLOW}‚ö† Robots.txt –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω{Colors.RESET}")
            return {'status': 'not_found'}

    def generate_report(self, pages: List[str], api_endpoints: Optional[List[str]] = None) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç"""
        print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
        print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞: {self.base_url}")
        print(f"{'='*60}{Colors.RESET}\n")

        report = {
            'base_url': self.base_url,
            'timestamp': datetime.now().isoformat(),
            'pages': {},
            'api': {},
            'sitemap': {},
            'robots': {},
            'summary': {}
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
        for page in pages:
            url = urljoin(self.base_url, page)
            report['pages'][page] = self.check_page_performance(url)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ API
        if api_endpoints:
            report['api'] = self.check_api_endpoints(api_endpoints)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ sitemap
        report['sitemap'] = self.check_sitemap()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ robots
        report['robots'] = self.check_robots()

        # –°–≤–æ–¥–∫–∞
        total_pages = len(report['pages'])
        ok_pages = sum(1 for p in report['pages'].values() if p.get('status') == 'ok')
        avg_load_time = sum(p.get('load_time', 0) for p in report['pages'].values()) / total_pages if total_pages > 0 else 0

        report['summary'] = {
            'total_pages': total_pages,
            'ok_pages': ok_pages,
            'error_pages': total_pages - ok_pages,
            'average_load_time': round(avg_load_time, 3),
            'status': 'ok' if ok_pages == total_pages else 'warning'
        }

        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
        print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
        print(f"–°–≤–æ–¥–∫–∞:{Colors.RESET}")
        print(f"  –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"  –£—Å–ø–µ—à–Ω–æ: {Colors.GREEN}{ok_pages}{Colors.RESET}")
        print(f"  –û—à–∏–±–æ–∫: {Colors.RED}{total_pages - ok_pages}{Colors.RESET}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {avg_load_time:.3f}s")
        print(f"{'='*60}\n")

        return report

def main():
    parser = argparse.ArgumentParser(description='–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞')
    parser.add_argument('--url', type=str, default='https://prizmabox.org',
                       help='–ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: http://localhost:3000)')
    parser.add_argument('--pages', type=str, nargs='+',
                       default=['/', '/service/ded-moroz', '/login', '/profile'],
                       help='–°—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
    parser.add_argument('--api', type=str, nargs='+',
                       default=['/api/videos/example/random', '/api/init'],
                       help='API endpoints –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
    parser.add_argument('--output', type=str, help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –æ—Ç—á—ë—Ç–∞')
    parser.add_argument('--timeout', type=int, default=30, help='–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö')

    args = parser.parse_args()

    checker = PerformanceChecker(args.url, timeout=args.timeout)
    report = checker.generate_report(args.pages, args.api)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"{Colors.GREEN}‚úì –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output}{Colors.RESET}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if report['summary']['status'] == 'ok':
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == '__main__':
    main()
